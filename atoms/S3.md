#### Summary of S3
S3 is an infinitely scaling object store. The backbone of AWS solutions.  99.999999999% durability - same across all storage classes. #AWSService 
#### S3 Details
- Low Latency of 100-20 0secs and high throughput 3500 writes/sec and 5500 reads/sec per prefix (probably sharding architecture by prefix as sharding key). You can improve performance across prefix by spreading files under prefix #performant 
- S3 Multi-Part upload is a parallel operation to boost performance. #performant 
- S3 multi-part upload can be done onto an Edge Location. S3 Transfer Acceleration can boost performance between Edge and the bucket. #performant 
- S3 Byte Range fetch can download n byte-ranges of files in parallel. #performant You can retry a specific byte range on failure. #resilient You can download only a byte range, for example, head of large file. #BestPractice 
- S3 Select and Glacier Select is a server-side filter that returns less data faster. #CostOptimized #performant 
- S3 Batch operations can perform a task on a list of object e.g. encrypt unencrypted file, or invoke a lambda function to perform tasks. Retries, reports, notifications are out of the box. S3 Inventory => S3 SQL => S3 Batch #WellArchitected 
- [[S3 Security]] consists of User-Based, Resource-Based (Bucket policies etc) and encryption options. #secure 
- [[S3_Website_Hosting]] permits hosting directly off of Buckets and [[CloudFront]] distributions
- S3 Versions protect against unintended deletes by rolling back. Suspending versioning will not delete the previous version. Version ID `null` is a valid "version". Delete the `delete marker` to restore a deleted file.
- Deletes can be protected with [[MFA]]
-  Cross Region Replication (CRR) and Same Region Replication (SRR).
- For very large transfers, it is recommended to use CLI `aws s3 sync`. It is not feasible to transfer large number of objects using the AWS Console
### Use Cases
- Storage: Hybrid Cloud Storage
- Disaster Recovery: Backup, Archive
- Hosting: Web Site, Media, Application
- Warehouse: Data Lakes, Big Data Analytics
### S3 with Requester Pays
The requester pays for the data transfer and the request, and the bucket owner pays for the data storage. However, the bucket owner is charged for the request under the following conditions:

-   The requester doesn't include the parameter `x-amz-request-payer` in the header (DELETE, GET, HEAD, POST, and PUT) or as a parameter (REST) in the request (HTTP code 403).
-   Request authentication fails (HTTP code 403).
-   The request is anonymous (HTTP code 403).
-   The request is a SOAP request.
### S3 Storage Class
![Summary](S3%20Storage%20Class.md#Summary)

## S3 Website
![Summary](S3_Website_Hosting.md#Summary)

## S3 Access Points
![Summary](S3%20Access%20Points.md#Summary)
#### Lifecycle Transition
S3 objects can be managed to go from Standard, to Standard IA, to One Zone or Glacier Instant, Glacier Flex and Deep Archive. There must be a **minimum 30 day gap** before an object can be moved to IA, and another 30 for OZ and another 30 for Glacier.
- If a company wants to restore deleted objects?
	- Implement versioning.
- If a company does not need objects frequently after 30 days...
	- Move objects to IA.
- If a company will not need objects for a year but may need instant access
	- Move to Glacier Instant
### S3 Storage Lens
- Organization wide visibility into object-storage usage and activity. Includes recommendations to optimize, secure and improve performance.
### S3 Replication
- Replications are performed asynchronously. Existing Objects are not replicated - unless S3 Batch Replication is enabled.
- Delete operations are not replicated to prevent malicious deletes from replications. Optionally `delete markers` can be replicated.
- No chaining of replication i.e. Bucket 1 replicated to Bucket 2 which is replicated to Bucket 3 wont move objects from B1 to B3.
### S3 Events
- S3 can send an event to [[Lambda]], [[SQS]] and [[SNS]] directly.
![[S3 Event to SNS SQS Lambda.png|256]]
- S3 can now send an event to [[EventBridge]] from where more sophisticated event driven architectures can be created. All events go to Event Bridge, from there various rules can be created to integrate to services.
![[S3 EventBridge.png|512]]
Fig. Event Bridge for Complex Event Processing
### S3 Access from VPC
- Either a Gateway Endpoint or an Interface Endpoint can be used from a [[VPC]].
- Gateway is preferred #CostOptimized - its free.
- Interface is only used from on-premises e.g. Site-to-site VPN or [[DX]] or from another region.
<<<<<<< HEAD
=======
### S3 Performance
S3 should be viewed as a large horizontally scalable solution. #performant 

- Scale storage connections horizontally - there is no limit to the number of connections to S3.
- Utilize multiple S3 paths for storage.
- Use Byte-Range fetches concurrently.
- Use aggressive timeouts and retry. A new faster route will be taken.

Source: AWS S3 Performance Best Practices [whitepaper](https://d1.awsstatic.com/whitepapers/AmazonS3BestPractices.pdf)
>>>>>>> 4269e823efd1d66f7d89a511cc387cec5418459d


## References

1. [AWS S3](https://aws.amazon.com/s3/)
2. [S3 Storage Lens](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-iam-policies.html) 